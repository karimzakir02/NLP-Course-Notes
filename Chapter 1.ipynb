{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 1 (Finding Words, Phrases, Names and Concepts)](https://course.spacy.io/en/chapter1)\n",
    "These are my notes for the first chapter of the advanced NLP [course](https://course.spacy.io/en/) provided by spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter contains:\n",
    "- Basics of text processing with spaCy\n",
    "- Data Structures\n",
    "- How to work with trained pipelines\n",
    "- How to use them to predict linguistic features in text\n",
    "\n",
    "### 1.1: Finding Words\n",
    "\n",
    "#### NLP\n",
    "\n",
    "At the center of spaCy is the object containing the processing pipeline, conventionally called 'nlp'. You can use this object like a function to analyze text. It contains all the different components in the pipeline _(what does this mean exactly?)_. Can be used with different languages; will contain different rules for tokenization based on the language. Over 60 languages available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") # spacy.blank method used when creating a blank pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc\n",
    "\n",
    "When you process text with the `nlp` object, spaCy creates a `Doc` object. It lets you access information about the text in a structured way, with no loss of information about the text. Behaves like a normal Python sequence and lets you iterate over its tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text) # print(token) will achieve the same result, but note that token itself isn't type str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token\n",
    "\n",
    "Token objects represent the tokens in a document, a word or a punctuation character. You can index `doc` to get a specific token. Token objects have attributes that let you access more info about a token, like `.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[1]\n",
    "\n",
    "print(token.text)\n",
    "\n",
    "dir(token)[28:35] # Some attribute examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span\n",
    "\n",
    "A Span object is a slice of a document, consisting of one or more tokens. Only a view of the `Doc` and doesn't contain any data itself _(data about what? Seems to contain data based on dir())_. To create one, you can use Python's slice notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Can I loop through it like a doc? Yes\n",
    "for token in span:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of current token:  0\n",
      "Text:  It\n",
      "Does it contain an alphanumeric character?  True\n",
      "Is it a punctuation character?  False\n",
      "Does it resemble a number?  False\n",
      "\n",
      "\n",
      "Index of current token:  1\n",
      "Text:  costs\n",
      "Does it contain an alphanumeric character?  True\n",
      "Is it a punctuation character?  False\n",
      "Does it resemble a number?  False\n",
      "\n",
      "\n",
      "Index of current token:  2\n",
      "Text:  $\n",
      "Does it contain an alphanumeric character?  False\n",
      "Is it a punctuation character?  False\n",
      "Does it resemble a number?  False\n",
      "\n",
      "\n",
      "Index of current token:  3\n",
      "Text:  50\n",
      "Does it contain an alphanumeric character?  False\n",
      "Is it a punctuation character?  False\n",
      "Does it resemble a number?  True\n",
      "\n",
      "\n",
      "Index of current token:  4\n",
      "Text:  .\n",
      "Does it contain an alphanumeric character?  False\n",
      "Is it a punctuation character?  True\n",
      "Does it resemble a number?  False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $50.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Index of current token: \", token.i)\n",
    "    print(\"Text: \", token.text)\n",
    "    print(\"Does it contain an alphanumeric character? \", token.is_alpha)\n",
    "    print(\"Is it a punctuation character? \", token.is_punct)\n",
    "    print(\"Does it resemble a number? \", token.like_num)\n",
    "    print(\"\\n\")\n",
    "    # Resembling a number can mean it is either expressed numerically (10) or alphanumerically (ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 50 is considered a single token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Ten dollars\")\n",
    "\n",
    "token = doc[0]\n",
    "print(token.like_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Trained Pipelines\n",
    "\n",
    "Trained Pipeline components have statistical models that enable spaCy to make predictions in context. For example, it can predict whether a word is a verb or a person's name. These pipelines are trained on large datasets of labeled example texts. They can also be updated with more examples to fine-tune their performance, to better perform on your data. \n",
    "\n",
    "A pipeline can be downloaded via the command prompt using the command: `python -m spacy download pipeline_name`. Example: `python -m spacy download en_core_web_sm`. \n",
    "\n",
    "The `en_core_web_sm` package is a small English pipeline that supports all core capabilities. The package provides binary weights that enable spaCy to make predictions. Also includes the vocabulary, meta information, and the configuration file used to train the pipeline. Tells spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # notice the use of the spacy.load method instead of spacy.blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try predicting part-of-speech tags, word types in context. After we load the pipeline and the document, for each token, we can print the text and the predicted part of speech tag (`pos_`). In spaCy, attributes that return strings usually end with an underscore; those without an underscore return an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict how words are related AKA syntactic dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dep_` returns the predicted dependency label. The `.head` attribute returns the synctatic head token; think of it as a parent token this word is attached to. \n",
    "\n",
    "Named entities are \"real world objects\". E.g: a person, an organization, or a country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "Apple is looking at buying U.K. startup for $1 billion\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents: # .ents returns a Span of all of the entities in the document\n",
    "    print(ent.text, ent.label_) # .label_ returns the entity label\n",
    "\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what a tag/label means, use `spacy.explain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "determiner\n",
      "noun, proper singular\n",
      "auxiliary\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"GPE\"))\n",
    "print(spacy.explain(\"det\"))\n",
    "print(spacy.explain(\"NNP\"))\n",
    "print(spacy.explain(\"AUX\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10: Rule-Based Matching\n",
    "\n",
    "Why not just use regular expressions? With spaCy, you can match on `Doc` objects, and not just strings; match on tokens and token attributes; use a mode's predictions; differentiate between duck (noun) and duck (verb). \n",
    "\n",
    "Match Patterns are lists of dictionaries, with each dictionairy describing a single token. So `len(pattern)` indicates how many tokens we are looking for. The keys are the names of token object attributes and the values are the expected values AKA what we want. Notice that we can have multiple keys in a single dictionairy, meaning we want a token to match multiple criteria. Consider the example below, where we are looking for two tokens with the text \"iPhone\" and \"X\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also match on other token attributes. We can look for two tokens whose lowercase forms equal \"iphone\" and \"x\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can write predictions based on the attributes predicted by the model. The below pattern would match a token with a lemma \"buy\" and a noun. Since the lemma is in base form, this would match stuff like \"buying milk\" or \"bought flowers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pattern, describing a token with multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"}, # a verb with the base lemma love\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we actually use a matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab) # Matcher is initialized with the shared vocab. More on this later - just remember to always pass it\n",
    "\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "# add a pattern to the Matcher. The first argument is a unique ID to identify which pattern was matched. The second arg is a list\n",
    "# of patterns\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Get a list of matches on a particular doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9528407286733565721\n",
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the matches:\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(match_id) # kinda weird that it returns the hash value of the pattern name, instead of the name itself, but whatever\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add quantifiers to our patterns. Consider the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above, the `?` makes the determiner optional. It will match both: \"bought a smartphone\" and \"buying apps\". To add quantifiers, use the \"OP\" keyword. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
