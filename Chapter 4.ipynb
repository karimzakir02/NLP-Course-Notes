{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 (Training a Neural Network Model) \n",
    "These are my notes for the fourth chapter of the advanced NLP [course](https://course.spacy.io/en/) provided by spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter contains:\n",
    "- How to update spaCy's statistical models\n",
    "- How to train your own model from scratch\n",
    "- Other tips & tricks regarding model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Training and Updating Models\n",
    "\n",
    "Why should we train and update our own model?\n",
    "- Better results on your specific domain, since the model will know your problem better\n",
    "- Classification schemes specific to your use-case/problem\n",
    "- Essential for text classification\n",
    "- Useful for named entity recognition\n",
    "- Less critical for part-of-speech tagging and dependency parsing. \n",
    "\n",
    "spaCy supports both: the training of new models and updates of existing ones. \n",
    "\n",
    "If we are not starting with a trained pipeline, we first initialize the weights randomly. Then, spaCy calls `nlp.update`, which predicts a batch of examples with the current weights. It checks the predictions against the correct answers, and decides how to change the weights to achieve better results. We make a small correction to the current weights and move on to the next batch. This continues in a cycle until the model stops improving. \n",
    "\n",
    "Let's take a look at the entity recognizer. It takes a document and predicts phrases and their labels in context. This means that the training data must include texts, the entities they contain, and the entity labels. Entities can't overlap, so each token can only be part of one entity. The easiest way to train an entity recognizer is to show the model a text and entity spans. It's also important for the model to learn words that aren't entities. The goal is to teach the model to recognize new entities in similar contexts, even if they weren't in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc1 = nlp(\"iPhone X is coming!\")\n",
    "doc1.ents = [Span(doc1, 0, 2, label=\"GADGET\")]\n",
    "\n",
    "# Also show examples that don't have the entities\n",
    "doc2 = nlp(\"I need a new phone! Any tips?\")\n",
    "doc2.ents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update an existing model, we can start with a few hundred to a few thousand examples. To train a new category, we will need up to a million. For example, spaCy's trained English pipelines were trained on 2 million words labelled with part-of-speech tags, dependencies and named entities. Training data is usually created by humans who assign labels to texts; this can be semi-automated using tools like `Matcher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc1, doc2]\n",
    "# Split data into train and test\n",
    "import random\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You typically want to store the data as files on disk to load them into spaCy's training process. The `DocBin` is a container for effeciently storing and serialzing `Doc` objects. You can instantiate it with a list of `Doc` objects and call its `to_disk` method. Typically use the .spacy extension for these files. Compare to other serialization tools like `pickle`, the `DocBin` is faster and produces smaller files, since it only stores the shared vocab once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "\n",
    "test_docbin = DocBin(docs=dev_docs)\n",
    "test_docbin.to_disk(\"./test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your data is already stored in some common format like CoNLL or IOB, spacy's `convert` command converts those files into spaCy's binary format. Also converts JSON files. \n",
    "\n",
    "$ python -m spacy convert ./train.gold.conll ./corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5: Configuring and running the training\n",
    "\n",
    "#### Configuring \n",
    "spaCy uses a config file (config.cfg) to determine settings like: how to init the `nlp` object, which components to add, how their internal implementations should be configured, settings for the training process, how to load the data, and hyperparameters. This means you only need to pass the config file when training the model. Helps with versioning and reproducability as well. An excerpt would look like as follows:\n",
    "\n",
    "\n",
    "```\n",
    "[nlp]\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\", \"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[nlp.tokenizer]\n",
    "@tokenizers = \"spacy.Tokenizer.v1\"\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "hidden_width = 64\n",
    "# And so on...\n",
    "```\n",
    "\n",
    "The above is split into sections, and nested sections are denoted with `.`, e.g: `[components.ner]`. Can reference Python objects using `@`. You can use this to customize different parts of the `nlp` object and training.\n",
    "\n",
    "Config files don't have to be written by hand; they can be generated by spaCy. You can use spaCy's built in `init config` command. The first argument is the file name, conventionally `config.cfg`. The argument `--lang` defines the language class; `--pipeline` lets you specify one or more command separated pipeline components to include. There's also an interactive [quick-start widget](https://spacy.io/usage/training#quickstart) on their website. An example command:\n",
    "\n",
    "`python -m spacy init config ./config.cfg --lang en --pipeline ner`\n",
    "\n",
    "#### Training\n",
    "\n",
    "To train a pipeline, all you need is the config file, and the training and testing data. The first argument of `spacy train` is the path to the config file. The `--output` argument lets you specify a directory for saving the final trained pipeline. Can also override different config settings on the command line. \n",
    "\n",
    "After training is done, you can easily load the pipeline using `spacy.load`. `model-last` is the last trained model, while `model-best` is the best trained model. \n",
    "\n",
    "#### Deploying\n",
    "To make it easy to deploy the pipeline, you can package it. The `spacy package` command takes the path to the pipeline and generates a Python package. Can also provide an optional name and version. \n",
    "\n",
    "### 4.10: Best Practices\n",
    "#### Problem 1: Models can 'forget' things\n",
    "When updating, existing models can overfit on new data, forgetting the stuff they learnt previously. For instance, if you're only updating it with examples of `WEBSITE`, it may \"forget\" other labels it previously predicted correctly â€“ like `PERSON`. This is also known as \"catastrophic forgetting\" problem. \n",
    "\n",
    "One of the solutions to this problem is to include previously correct predictions. spaCy can help by running the existing model over data and extracting the entity spans you care about. You can then mix those examples in with yout existing data and update the model with annotations of all labels. \n",
    "\n",
    "#### Problem 2: Models can't learn everything\n",
    "spaCy's models make predictions based on local context - for example, for named entities, surrounding words are important. If the decision is difficult to make based on the context, the model will have a hard time. The label scheme also needs to be consistent and not too specific; `CLOTHING` might be a better label than `ADULT_CLOTHING` and `CHILDREN_CLOTHING`. \n",
    "\n",
    "As such, you should plan your label scheme carefully by:\n",
    "- Picking categories reflected in local context\n",
    "- Making them more generic\n",
    "- Use rules to go from generic labels to specific categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
