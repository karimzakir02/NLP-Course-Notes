{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 3 (Processing Pipelines)](https://course.spacy.io/en/chapter3)\n",
    "These are my notes for the third chapter of the advanced NLP [course](https://course.spacy.io/en/) provided by spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter contains:\n",
    "- What happens under the hood when you process text\n",
    "- How to write your own components and add them to the pipeline\n",
    "- How to add custom attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Processing Pipelines\n",
    "A processing pipeline is a series of functions applied to a document to add attributes like part-of-speech tags, dependency labels, or named entities. \n",
    "\n",
    "#### What happens when you call nlp?\n",
    "First, the tokenizer is applied to turn the string of text into a `Doc` object. Next, a series of pipeline components is applied to the object in order. In this case, the tagger, then the parser, and then the entity recognizer. Afterwards, the object itself is returned. \n",
    "\n",
    "The Pipeline for each model is defined in the models's `config.cfg` file. This file tells spaCy which components to instantiate and how to configure them. These built-in components that make predictions also need binary data, which is included in the pipeline package and is loaded into the component when you load the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001B48EAF5040>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001B48EAF53A0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001B48E969D60>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001B48EB6B080>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001B48EB50E40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001B48E2B70B0>)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print(nlp.pipe_names) # you can see the names of the components like this\n",
    "print(nlp.pipeline) # this returns the name of the component and the function the component applies to the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Custom Pipeline Components\n",
    "Custom pipeline components let you add your own function to the spaCy pipeline and add more data to the `Doc` object. You can use your own custom functions/components to add custom data to the document and its tokens, or to update built-in attributes. \n",
    "\n",
    "Fundamentally, a component is a function/callable that takes a `Doc`, modifies it and returns it to be processed by the next component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_component\") # register it using this decorator\n",
    "def custom_component_function(doc): # take the doc as an argument\n",
    "    # Change/process the document \n",
    "    return doc # make sure to return the doc, so it's processed by the next component\n",
    "\n",
    "nlp.add_pipe(\"custom_component\") # add the component to the pipeline\n",
    "# the above method takes at least one arg - the component's name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `add_pipe` function you can specify where in the pipeline should the component be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"custom_component\", last=True) # add the component to the end - default behavior\n",
    "nlp.add_pipe(\"custom_component\", first=True) # add the component to the front\n",
    "nlp.add_pipe(\"custom_component\", before=\"ner\") # add the component before another component - ner in this case\n",
    "nlp.add_pipe(\"custom_component\", after=\"tagger\") # add the component after another component - tagger in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can't tweak other components with this.\n",
    "\n",
    "### 3.8: Extension Attributes\n",
    "You can set custom attributes to the doc, its tokens/spans. It can be added once or computer dynamically. Custom attributes are available via the `._` property, to make it clear they are custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# First, you gotta register the extension with the associated component:\n",
    "Doc.set_extension(\"title\", default=None) # first arg is the attribute name, other args define how the property\n",
    "Token.set_extension(\"is_color\", default=False) # can be computed or what the default value should be\n",
    "Span.set_extension(\"has_color\", default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then access them as below:\n",
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of extensions:\n",
    "1. Attribute Extensions\n",
    "2. Property Extensions\n",
    "3. Method Extensions\n",
    "\n",
    "#### Attribute Extensions\n",
    "Attribute extensions set a default value that can be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension(\"is_color\", default=False, force=True) # force was added to override the previous extension\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property Extensions\n",
    "Property extensions work like properties in Python: they can define a getter function and an optional setter. The getter funciton is only called when you retrieve the attribute. This allows you to set the value dynamically and take custom attributes into account. Getter functions take one argument - the object. We provide the function via the getter keyword and register the extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a getter function:\n",
    "def get_is_color(token): # take the object as an argument\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True) # pass the function as a getter\n",
    "\n",
    "doc = nlp(\"The sky is blue\")\n",
    "doc[3]._.is_color # access the custom attribute via _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Span` extensions should almost always use a getter. Otherwise, you would need to update every possible span in the document. Since you can have a lot of spans, it's best to calculate these things at runtime, so use a property extension.\n",
    "\n",
    "\n",
    "#### Method Extensions\n",
    "Method extensions make the extension attribute a callable method, allowing you to pass one or more arguments to it and compute attribute values dynamically, based on a certain argument/setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def has_token(doc, token_text): # first argument is the object still, and other args are also possible\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "Doc.set_extension(\"has_token\", method=has_token) # register the extension, and pass the method via the method arg\n",
    "\n",
    "doc = nlp(\"The sky is blue\")\n",
    "print(doc._.has_token(\"blue\")) # it's a callable, and you only need to pass the other arguments, not the object\n",
    "print(doc._.has_token(\"cloud\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13: Scaling and Performance\n",
    "There are some tricks that can be used to optimize the performance of the components in the pipeline. \n",
    "\n",
    "If you need to process a lot of texts and create a lot of `Doc` objects in a row, the `nlp.pipe` method can speed this up significantly. It processes texts as a stream and yields `Doc` objects. It's much faster since it bunches up the texts. Since the method is a generator, it yields the objects, so to get a list of `Doc`'s, make sure to convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD:\n",
    "docs = [nlp(text) for text in texts]\n",
    "# GOOD:\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp.pipe` is also useful for passing additional metadata, since it supports passing text-context/metadata tuples/pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15 1\n",
      "And another text 16 2\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"], context[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This context/metadata can even become custom attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using only the Tokenizer\n",
    "\n",
    "Sometimes you might have the model loaded to do other processing, but you only need to the tokenizer for one particular text. Running the entire pipeline is slow, because you'll be getting predictions from the model you won't need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD:\n",
    "doc = nlp(\"Hello World!\")\n",
    "# GOOD:\n",
    "doc = nlp.make_doc(\"Hello World!\") # this will return a tokenized doc, but won't apply any other components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disabling Pipeline Components\n",
    "You can disable certain components of the pipeline in case you don't need them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "# Since we are only using context manager, after we are done, other components will be automatically enabled\n",
    "# Also, accepts the enable keyword to only enable a few components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
